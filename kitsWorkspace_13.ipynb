{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/1/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRAGNA\\AppData\\Local\\Temp\\ipykernel_17392\\3045113456.py:171: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed locating file data/8: file not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 238\u001b[0m\n\u001b[0;32m    235\u001b[0m mask_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/kits23/split_dataset/train/masks/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 171\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(image_dir, mask_dir, epochs, batch_size, lr, num_folds, checkpoint_dir)\u001b[0m\n\u001b[0;32m    169\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_checkpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(checkpoint_path):\n\u001b[1;32m--> 171\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    173\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1361\u001b[0m             opened_zipfile,\n\u001b[0;32m   1362\u001b[0m             map_location,\n\u001b[0;32m   1363\u001b[0m             pickle_module,\n\u001b[0;32m   1364\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1365\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1366\u001b[0m         )\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[0;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:1812\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1811\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1812\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:1772\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1769\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset : storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[0;32m   1770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1771\u001b[0m     storage \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1772\u001b[0m         \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1773\u001b[0m         \u001b[38;5;241m.\u001b[39m_typed_storage()\n\u001b[0;32m   1774\u001b[0m         \u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[0;32m   1775\u001b[0m     )\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: PytorchStreamReader failed locating file data/8: file not found"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset class to handle image and mask loading\n",
    "class KidneySegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, mask_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        \n",
    "        # Collect all case folders (e.g., '00000', '00001', ...)\n",
    "        self.case_folders = os.listdir(image_dir)\n",
    "        self.case_folders = [folder for folder in self.case_folders if os.path.isdir(os.path.join(image_dir, folder))]\n",
    "        \n",
    "        # Calculate total slices across all cases and store slice counts\n",
    "        self.slice_counts = []\n",
    "        for case_id in self.case_folders:\n",
    "            case_image_dir = os.path.join(self.image_dir, case_id)\n",
    "            # Count number of images in the case\n",
    "            num_slices = len([f for f in os.listdir(case_image_dir) if f.endswith('.png')])  # assuming images are .png\n",
    "            self.slice_counts.append(num_slices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # The length is the total number of slices in all case folders\n",
    "        return sum(self.slice_counts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Determine the case folder and slice number for the given index\n",
    "        cumulative_slices = 0\n",
    "        for i, num_slices in enumerate(self.slice_counts):\n",
    "            cumulative_slices += num_slices\n",
    "            if idx < cumulative_slices:\n",
    "                case_id = self.case_folders[i]\n",
    "                slice_id = idx - (cumulative_slices - num_slices)\n",
    "                break  # Slice indexing\n",
    "        \n",
    "        # Build the paths for the image and mask slice\n",
    "        img_files = sorted([f for f in os.listdir(os.path.join(self.image_dir, case_id)) if f.endswith('.png')])  # Assuming PNG files\n",
    "        mask_files = sorted([f for f in os.listdir(os.path.join(self.mask_dir, case_id)) if f.endswith('.png')])  # Assuming PNG files\n",
    "        \n",
    "        img_path = os.path.join(self.image_dir, case_id, img_files[slice_id])\n",
    "        mask_path = os.path.join(self.mask_dir, case_id, mask_files[slice_id])\n",
    "\n",
    "        # Load the image and mask\n",
    "        img = Image.open(img_path).convert(\"RGB\")  # Convert to RGB (3 channels)\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale (1 channel)\n",
    "        \n",
    "        # Apply any transformations if provided\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)  # The mask transformation will not change the number of channels\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "# EfficientNet-B5 Encoder (from torchvision)\n",
    "class EfficientNetB5Encoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(EfficientNetB5Encoder, self).__init__()\n",
    "        # Load the pre-trained EfficientNet-B5 model from torchvision\n",
    "        self.encoder = models.efficientnet_b5(pretrained=pretrained)\n",
    "        \n",
    "        # Remove the classification head (fully connected layers) to retain features\n",
    "        self.encoder = nn.Sequential(*list(self.encoder.children())[:-1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features from the encoder\n",
    "        features = self.encoder(x)\n",
    "        return features\n",
    "\n",
    "# FPN Decoder\n",
    "class FPNDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FPNDecoder, self).__init__()\n",
    "        # FPN typically uses several layers to combine features from different scales\n",
    "        self.conv1 = nn.Conv2d(in_channels, 256, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        \n",
    "        # Instead of fixed upsampling, use adaptive upsampling\n",
    "        self.upsample = nn.Upsample(size=(512, 512), mode='bilinear', align_corners=True)  # Adaptive resizing to (512, 512)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Decode and combine features from multiple scales\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        # Upsample the output to match the input size (512x512)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "# Final model combining EfficientNet-B5 and FPN\n",
    "class EfficientNetFPN(nn.Module):\n",
    "    def __init__(self, out_channels=1, pretrained=True):\n",
    "        super(EfficientNetFPN, self).__init__()\n",
    "        # Encoder: EfficientNetB5\n",
    "        self.encoder = EfficientNetB5Encoder(pretrained=pretrained)\n",
    "        \n",
    "        # Decoder: FPN\n",
    "        self.decoder = FPNDecoder(in_channels=2048, out_channels=out_channels)  # EfficientNet-B5 has 2048 channels in the final layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through encoder and decoder\n",
    "        features = self.encoder(x)\n",
    "        out = self.decoder(features)\n",
    "        return out\n",
    "\n",
    "# Loss Function: Binary Cross-Entropy (BCE)\n",
    "def bce_loss(output, target):\n",
    "    return nn.BCEWithLogitsLoss()(output, target)\n",
    "\n",
    "# Training loop with 5-fold cross-validation\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(image_dir, mask_dir, epochs=50, batch_size=2, lr=0.001, num_folds=5, checkpoint_dir='checkpoints'):\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize the dataset\n",
    "    dataset = KidneySegmentationDataset(image_dir=image_dir, mask_dir=mask_dir, transform=transform, mask_transform=mask_transform)\n",
    "\n",
    "    # Set up cross-validation\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset), start=1):\n",
    "        print(f\"Training fold {fold}/{num_folds}...\")\n",
    "\n",
    "        # Split dataset\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        # Data loaders\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize model, optimizer, and loss function\n",
    "        model = EfficientNetFPN(out_channels=1, pretrained=True)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Load checkpoint if exists\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"fold_{fold}_checkpoint.pth\")\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            start_batch = checkpoint['batch']\n",
    "            print(f\"Resuming from epoch {start_epoch + 1}, batch {start_batch + 1}\")\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            start_batch = 0\n",
    "            print(\"Starting from scratch\")\n",
    "            \n",
    "            \n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for batch_idx, (images, masks) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"), start=start_batch):\n",
    "                # Skip already processed batches\n",
    "                if batch_idx < start_batch:\n",
    "                    continue\n",
    "\n",
    "                # Move to device\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = bce_loss(outputs, masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                # Save progress after each batch\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': batch_idx,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, checkpoint_path)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for images, masks in val_loader:\n",
    "                    images, masks = images.to(device), masks.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = bce_loss(outputs, masks)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            print(f\"Validation Loss after Epoch {epoch + 1}: {val_loss / len(val_loader)}\")\n",
    "\n",
    "            # Reset start_batch for the next epoch\n",
    "            start_batch = 0\n",
    "\n",
    "\n",
    "# Set directories for your image and mask data\n",
    "image_dir = \"E:/kits23/split_dataset/train/images/\"\n",
    "mask_dir = \"E:/kits23/split_dataset/train/masks/\"\n",
    "\n",
    "# Train the model\n",
    "train_model(image_dir, mask_dir, epochs=50, batch_size=2, lr=0.001, num_folds=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training : attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62088 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 173\u001b[0m\n\u001b[0;32m    171\u001b[0m image_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/kits23/split_dataset/train/images/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m mask_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/kits23/split_dataset/train/masks/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 144\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(image_dir, mask_dir, epochs, batch_size, lr, num_folds, checkpoint_dir)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m<\u001b[39m start_batch:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m images, masks \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, masks\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    145\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    146\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset class to handle image and mask loading\n",
    "class KidneySegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, mask_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        \n",
    "        self.case_folders = [folder for folder in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, folder))]\n",
    "        self.slice_counts = [\n",
    "            len([f for f in os.listdir(os.path.join(image_dir, case)) if f.endswith('.png')])\n",
    "            for case in self.case_folders\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(self.slice_counts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cumulative_slices = 0\n",
    "        for i, num_slices in enumerate(self.slice_counts):\n",
    "            cumulative_slices += num_slices\n",
    "            if idx < cumulative_slices:\n",
    "                case_id = self.case_folders[i]\n",
    "                slice_id = idx - (cumulative_slices - num_slices)\n",
    "                break\n",
    "        \n",
    "        img_files = sorted([f for f in os.listdir(os.path.join(self.image_dir, case_id)) if f.endswith('.png')])\n",
    "        mask_files = sorted([f for f in os.listdir(os.path.join(self.mask_dir, case_id)) if f.endswith('.png')])\n",
    "        \n",
    "        img_path = os.path.join(self.image_dir, case_id, img_files[slice_id])\n",
    "        mask_path = os.path.join(self.mask_dir, case_id, mask_files[slice_id])\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "# EfficientNet-B5 Encoder (from torchvision)\n",
    "class EfficientNetB5Encoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(EfficientNetB5Encoder, self).__init__()\n",
    "        self.encoder = models.efficientnet_b5(pretrained=pretrained)\n",
    "        self.encoder = nn.Sequential(*list(self.encoder.children())[:-1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# FPN Decoder\n",
    "class FPNDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FPNDecoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 256, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        self.upsample = nn.Upsample(size=(512, 512), mode='bilinear', align_corners=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        return self.upsample(x)\n",
    "\n",
    "# Final model combining EfficientNet-B5 and FPN\n",
    "class EfficientNetFPN(nn.Module):\n",
    "    def __init__(self, out_channels=1, pretrained=True):\n",
    "        super(EfficientNetFPN, self).__init__()\n",
    "        self.encoder = EfficientNetB5Encoder(pretrained=pretrained)\n",
    "        self.decoder = FPNDecoder(in_channels=2048, out_channels=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.decoder(features)\n",
    "\n",
    "# Binary Cross-Entropy Loss\n",
    "def bce_loss(output, target):\n",
    "    return nn.BCEWithLogitsLoss()(output, target)\n",
    "\n",
    "# Safe Save Function\n",
    "def safe_save(state, filepath):\n",
    "    temp_path = filepath + \".tmp\"\n",
    "    torch.save(state, temp_path)\n",
    "    os.rename(temp_path, filepath)\n",
    "\n",
    "# Training function with checkpointing and cross-validation\n",
    "def train_model(image_dir, mask_dir, epochs=50, batch_size=2, lr=0.001, num_folds=5, checkpoint_dir='checkpoints'):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    mask_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    dataset = KidneySegmentationDataset(image_dir, mask_dir, transform=transform, mask_transform=mask_transform)\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset), start=1):\n",
    "        print(f\"Training fold {fold}/{num_folds}...\")\n",
    "\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = EfficientNetFPN(out_channels=1, pretrained=True).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"fold_{fold}_checkpoint.pth\")\n",
    "        \n",
    "        start_epoch, start_batch = 0, 0\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            try:\n",
    "                checkpoint = torch.load(checkpoint_path)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                start_epoch = checkpoint['epoch'] + 1\n",
    "                start_batch = checkpoint['batch'] + 1\n",
    "                print(f\"Resumed from fold {fold}, epoch {start_epoch}, batch {start_batch}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load checkpoint: {e}. Starting fold {fold} from scratch.\")\n",
    "                os.remove(checkpoint_path)\n",
    "\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for batch_idx, (images, masks) in enumerate(tqdm(train_loader), start=start_batch):\n",
    "                if batch_idx < start_batch:\n",
    "                    continue\n",
    "                images, masks = images.to('cuda'), masks.to('cuda')\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = bce_loss(outputs, masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                safe_save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': batch_idx,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, checkpoint_path)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "            start_batch = 0\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for images, masks in val_loader:\n",
    "                    images, masks = images.to('cuda'), masks.to('cuda')\n",
    "                    outputs = model(images)\n",
    "                    loss = bce_loss(outputs, masks)\n",
    "                    val_loss += loss.item()\n",
    "            print(f\"Validation Loss after Epoch {epoch + 1}: {val_loss / len(val_loader)}\")\n",
    "\n",
    "# Set directories and train\n",
    "image_dir = \"E:/kits23/split_dataset/train/images/\"\n",
    "mask_dir = \"E:/kits23/split_dataset/train/masks/\"\n",
    "train_model(image_dir, mask_dir, epochs=50, batch_size=2, lr=0.001, num_folds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRAGNA\\AppData\\Local\\Temp\\ipykernel_17392\\2391620599.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming fold 1 from epoch 1, batch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 67/62088 [04:23<67:45:02,  3.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 163\u001b[0m\n\u001b[0;32m    161\u001b[0m image_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/kits23/split_dataset/train/images/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m mask_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/kits23/split_dataset/train/masks/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 163\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 140\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(image_dir, mask_dir, epochs, batch_size, lr, num_folds, checkpoint_dir)\u001b[0m\n\u001b[0;32m    138\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m    139\u001b[0m loss \u001b[38;5;241m=\u001b[39m bce_loss(outputs, masks)\n\u001b[1;32m--> 140\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    142\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Dataset class to handle image and mask loading\n",
    "class KidneySegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, mask_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        \n",
    "        self.case_folders = [folder for folder in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, folder))]\n",
    "        self.slice_counts = []\n",
    "        for case_id in self.case_folders:\n",
    "            case_image_dir = os.path.join(self.image_dir, case_id)\n",
    "            num_slices = len([f for f in os.listdir(case_image_dir) if f.endswith('.png')])\n",
    "            self.slice_counts.append(num_slices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return sum(self.slice_counts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cumulative_slices = 0\n",
    "        for i, num_slices in enumerate(self.slice_counts):\n",
    "            cumulative_slices += num_slices\n",
    "            if idx < cumulative_slices:\n",
    "                case_id = self.case_folders[i]\n",
    "                slice_id = idx - (cumulative_slices - num_slices)\n",
    "                break\n",
    "        \n",
    "        img_files = sorted([f for f in os.listdir(os.path.join(self.image_dir, case_id)) if f.endswith('.png')])\n",
    "        mask_files = sorted([f for f in os.listdir(os.path.join(self.mask_dir, case_id)) if f.endswith('.png')])\n",
    "        \n",
    "        img_path = os.path.join(self.image_dir, case_id, img_files[slice_id])\n",
    "        mask_path = os.path.join(self.mask_dir, case_id, mask_files[slice_id])\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "# EfficientNet-B5 Encoder\n",
    "class EfficientNetB5Encoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(EfficientNetB5Encoder, self).__init__()\n",
    "        self.encoder = models.efficientnet_b5(weights='EfficientNet_B5_Weights.DEFAULT' if pretrained else None)\n",
    "        self.encoder = nn.Sequential(*list(self.encoder.children())[:-2])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# FPN Decoder\n",
    "class FPNDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FPNDecoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 256, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        self.upsample = nn.Upsample(size=(512, 512), mode='bilinear', align_corners=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "# Combined EfficientNet-FPN model\n",
    "class EfficientNetFPN(nn.Module):\n",
    "    def __init__(self, out_channels=1, pretrained=True):\n",
    "        super(EfficientNetFPN, self).__init__()\n",
    "        self.encoder = EfficientNetB5Encoder(pretrained=pretrained)\n",
    "        self.decoder = FPNDecoder(in_channels=2048, out_channels=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.decoder(features)\n",
    "\n",
    "# Loss function\n",
    "def bce_loss(output, target):\n",
    "    return nn.BCEWithLogitsLoss()(output, target)\n",
    "\n",
    "# Training function with checkpointing and cross-validation\n",
    "def train_model(image_dir, mask_dir, epochs=50, batch_size=2, lr=0.001, num_folds=5, checkpoint_dir='checkpoints'):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    mask_transform = transforms.ToTensor()\n",
    "    dataset = KidneySegmentationDataset(image_dir, mask_dir, transform=transform, mask_transform=mask_transform)\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset), start=1):\n",
    "        print(f\"Training fold {fold}/{num_folds}...\")\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        model = EfficientNetFPN(out_channels=1, pretrained=True).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"fold_{fold}_checkpoint.pth\")\n",
    "        start_epoch, start_batch = 0, 0\n",
    "        \n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            start_batch = checkpoint['batch']\n",
    "            print(f\"Resuming fold {fold} from epoch {start_epoch + 1}, batch {start_batch + 1}\")\n",
    "        else:\n",
    "            print(f\"Starting fold {fold} from scratch.\")\n",
    "        \n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for batch_idx, (images, masks) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"), start=start_batch):\n",
    "                if batch_idx < start_batch:\n",
    "                    continue\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = bce_loss(outputs, masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': batch_idx,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, checkpoint_path)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "            start_batch = 0\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for images, masks in val_loader:\n",
    "                    images, masks = images.to(device), masks.to(device)\n",
    "                    outputs = model(images)\n",
    "                    val_loss += bce_loss(outputs, masks).item()\n",
    "            print(f\"Validation Loss after Epoch {epoch + 1}: {val_loss / len(val_loader)}\")\n",
    "\n",
    "image_dir = \"E:/kits23/split_dataset/train/images/\"\n",
    "mask_dir = \"E:/kits23/split_dataset/train/masks/\"\n",
    "train_model(image_dir, mask_dir, epochs=50, batch_size=2, lr=0.001, num_folds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRAGNA\\AppData\\Local\\Temp\\ipykernel_17392\\3123974498.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming fold 1 from epoch 1, batch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 15/62088 [00:07<129:15:10,  7.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 164\u001b[0m\n\u001b[0;32m    162\u001b[0m image_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/kits23/split_dataset/train/images/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m mask_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/kits23/split_dataset/train/masks/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 164\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 141\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(image_dir, mask_dir, epochs, batch_size, lr, num_folds, checkpoint_dir)\u001b[0m\n\u001b[0;32m    139\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m    140\u001b[0m loss \u001b[38;5;241m=\u001b[39m bce_loss(outputs, masks)\n\u001b[1;32m--> 141\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    143\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Dataset class to handle image and mask loading\n",
    "class KidneySegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, mask_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "        self.case_folders = [folder for folder in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, folder))]\n",
    "        self.slice_counts = []\n",
    "        for case_id in self.case_folders:\n",
    "            case_image_dir = os.path.join(self.image_dir, case_id)\n",
    "            num_slices = len([f for f in os.listdir(case_image_dir) if f.endswith('.png')])\n",
    "            self.slice_counts.append(num_slices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.slice_counts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cumulative_slices = 0\n",
    "        for i, num_slices in enumerate(self.slice_counts):\n",
    "            cumulative_slices += num_slices\n",
    "            if idx < cumulative_slices:\n",
    "                case_id = self.case_folders[i]\n",
    "                slice_id = idx - (cumulative_slices - num_slices)\n",
    "                break\n",
    "\n",
    "        img_files = sorted([f for f in os.listdir(os.path.join(self.image_dir, case_id)) if f.endswith('.png')])\n",
    "        mask_files = sorted([f for f in os.listdir(os.path.join(self.mask_dir, case_id)) if f.endswith('.png')])\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, case_id, img_files[slice_id])\n",
    "        mask_path = os.path.join(self.mask_dir, case_id, mask_files[slice_id])\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "# EfficientNet-B5 Encoder\n",
    "class EfficientNetB5Encoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(EfficientNetB5Encoder, self).__init__()\n",
    "        self.encoder = models.efficientnet_b5(weights='EfficientNet_B5_Weights.DEFAULT' if pretrained else None)\n",
    "        self.encoder = nn.Sequential(*list(self.encoder.children())[:-2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# FPN Decoder\n",
    "class FPNDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FPNDecoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 256, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        self.upsample = nn.Upsample(size=(512, 512), mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "# Combined EfficientNet-FPN model\n",
    "class EfficientNetFPN(nn.Module):\n",
    "    def __init__(self, out_channels=1, pretrained=True):\n",
    "        super(EfficientNetFPN, self).__init__()\n",
    "        self.encoder = EfficientNetB5Encoder(pretrained=pretrained)\n",
    "        self.decoder = FPNDecoder(in_channels=2048, out_channels=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.decoder(features)\n",
    "\n",
    "# Loss function\n",
    "def bce_loss(output, target):\n",
    "    return nn.BCEWithLogitsLoss()(output, target)\n",
    "\n",
    "# Training function with checkpointing and cross-validation\n",
    "def train_model(image_dir, mask_dir, epochs=50, batch_size=2, lr=0.001, num_folds=5, checkpoint_dir='checkpoints'):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    mask_transform = transforms.ToTensor()\n",
    "    dataset = KidneySegmentationDataset(image_dir, mask_dir, transform=transform, mask_transform=mask_transform)\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset), start=1):\n",
    "        print(f\"Training fold {fold}/{num_folds}...\")\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        model = EfficientNetFPN(out_channels=1, pretrained=True).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"fold_{fold}_checkpoint.pth\")\n",
    "        start_epoch, start_batch = 0, 0\n",
    "\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            start_batch = checkpoint['batch']\n",
    "            print(f\"Resuming fold {fold} from epoch {start_epoch + 1}, batch {start_batch + 1}\")\n",
    "        else:\n",
    "            print(f\"Starting fold {fold} from scratch.\")\n",
    "\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", initial=start_batch, total=len(train_loader))\n",
    "            for batch_idx, (images, masks) in enumerate(pbar, start=start_batch):\n",
    "                if batch_idx < start_batch:\n",
    "                    continue\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = bce_loss(outputs, masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': batch_idx,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, checkpoint_path)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "            start_batch = 0\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for images, masks in val_loader:\n",
    "                    images, masks = images.to(device), masks.to(device)\n",
    "                    outputs = model(images)\n",
    "                    val_loss += bce_loss(outputs, masks).item()\n",
    "            print(f\"Validation Loss after Epoch {epoch + 1}: {val_loss / len(val_loader)}\")\n",
    "\n",
    "image_dir = \"E:/kits23/split_dataset/train/images/\"\n",
    "mask_dir = \"E:/kits23/split_dataset/train/masks/\"\n",
    "train_model(image_dir, mask_dir, epochs=50, batch_size=2, lr=0.001, num_folds=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
