{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderEfficientNetB5(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(EncoderEfficientNetB5, self).__init__()\n",
    "        # Load EfficientNet-B5\n",
    "        self.encoder = EfficientNet.from_pretrained('efficientnet-b5') if pretrained else EfficientNet.from_name('efficientnet-b5')\n",
    "        # Extract blocks (stages)\n",
    "        self.blocks = self.encoder._blocks\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Store features from different stages\n",
    "        features = []\n",
    "        x = self.encoder._conv_stem(x)  # Initial convolution\n",
    "        x = self.encoder._bn0(x)       # Batch normalization\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            features.append(x)         # Save intermediate features\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderFPN(nn.Module):\n",
    "    def __init__(self, encoder_channels, decoder_channels):\n",
    "        super(DecoderFPN, self).__init__()\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        self.lat_convs = nn.ModuleList()\n",
    "\n",
    "        # Build lateral and upsampling convolutions\n",
    "        for in_channels in encoder_channels:\n",
    "            self.lat_convs.append(nn.Conv2d(in_channels, decoder_channels, kernel_size=1))\n",
    "            self.up_convs.append(nn.Conv2d(decoder_channels, decoder_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "    def forward(self, encoder_features):\n",
    "        # Start from the deepest feature and go upwards\n",
    "        x = self.lat_convs[-1](encoder_features[-1])  # Lateral convolution for the deepest feature\n",
    "        outputs = [x]  # Collect outputs\n",
    "\n",
    "        # Iterate through the encoder features in reverse order\n",
    "        for i in range(len(encoder_features) - 2, -1, -1):\n",
    "            x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)  # Upsample\n",
    "            lateral = self.lat_convs[i](encoder_features[i])  # Apply lateral convolution\n",
    "            x = x + lateral  # Add lateral and upsampled feature\n",
    "            x = self.up_convs[i](x)  # Apply up convolution\n",
    "            outputs.append(x)\n",
    "        \n",
    "        return outputs[::-1]  # Reverse to match spatial resolution order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetFPN(nn.Module):\n",
    "    def __init__(self, num_classes, encoder_channels, decoder_channels=256):\n",
    "        super(EfficientNetFPN, self).__init__()\n",
    "        self.encoder = EncoderEfficientNetB5(pretrained=True)\n",
    "        self.decoder = DecoderFPN(encoder_channels, decoder_channels)\n",
    "        self.final_conv = nn.Conv2d(decoder_channels, num_classes, kernel_size=1)  # Final 1x1 convolution\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_features = self.encoder(x)\n",
    "        decoder_features = self.decoder(encoder_features)\n",
    "        x = decoder_features[0]  # Use the highest-resolution output\n",
    "        x = self.final_conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "# Encoder channels for EfficientNet-B5\n",
    "encoder_channels = [48, 144, 240, 384, 2048]  # Output channels from each stage\n",
    "\n",
    "# Initialize the model\n",
    "model = EfficientNetFPN(num_classes=1, encoder_channels=encoder_channels) #num_classes = 2?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BCEWithLogitsLossCustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCEWithLogitsLossCustom, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()  # Automatically applies sigmoid\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return self.bce_loss(inputs, targets)\n",
    "\n",
    "# Example usage:\n",
    "# loss_fn = BCEWithLogitsLossCustom()\n",
    "# loss = loss_fn(output, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(BCEFocalLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Flatten the inputs and targets\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        # Compute BCE loss\n",
    "        bce_loss = self.bce_loss(inputs, targets)\n",
    "\n",
    "        # Compute Focal Loss modulation factor\n",
    "        pt = torch.exp(-bce_loss)  # Probability of the true class\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Example usage:\n",
    "# combined_loss = BCEFocalLoss(alpha=0.25, gamma=2)\n",
    "# loss = combined_loss(output, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking case folders...\n",
      "\n",
      "Processing case folder: 00000\n",
      "\n",
      "Processing case folder: 00001\n",
      "\n",
      "Processing case folder: 00002\n",
      "\n",
      "Processing case folder: 00003\n",
      "\n",
      "Processing case folder: 00004\n",
      "\n",
      "Processing case folder: 00005\n",
      "\n",
      "Processing case folder: 00006\n",
      "\n",
      "Processing case folder: 00007\n",
      "\n",
      "Processing case folder: 00008\n",
      "\n",
      "Processing case folder: 00009\n",
      "\n",
      "Processing case folder: 00010\n",
      "\n",
      "Processing case folder: 00011\n",
      "\n",
      "Processing case folder: 00012\n",
      "\n",
      "Processing case folder: 00013\n",
      "\n",
      "Processing case folder: 00014\n",
      "\n",
      "Processing case folder: 00015\n",
      "\n",
      "Processing case folder: 00016\n",
      "\n",
      "Processing case folder: 00017\n",
      "\n",
      "Processing case folder: 00018\n",
      "\n",
      "Processing case folder: 00019\n",
      "\n",
      "Processing case folder: 00020\n",
      "\n",
      "Processing case folder: 00021\n",
      "\n",
      "Processing case folder: 00022\n",
      "\n",
      "Processing case folder: 00023\n",
      "\n",
      "Processing case folder: 00024\n",
      "\n",
      "Processing case folder: 00025\n",
      "\n",
      "Processing case folder: 00026\n",
      "\n",
      "Processing case folder: 00027\n",
      "\n",
      "Processing case folder: 00028\n",
      "\n",
      "Processing case folder: 00029\n",
      "\n",
      "Processing case folder: 00030\n",
      "\n",
      "Processing case folder: 00031\n",
      "\n",
      "Processing case folder: 00032\n",
      "\n",
      "Processing case folder: 00033\n",
      "\n",
      "Processing case folder: 00034\n",
      "\n",
      "Processing case folder: 00035\n",
      "\n",
      "Processing case folder: 00036\n",
      "\n",
      "Processing case folder: 00037\n",
      "\n",
      "Processing case folder: 00038\n",
      "\n",
      "Processing case folder: 00039\n",
      "\n",
      "Processing case folder: 00040\n",
      "\n",
      "Processing case folder: 00041\n",
      "\n",
      "Processing case folder: 00042\n",
      "\n",
      "Processing case folder: 00043\n",
      "\n",
      "Processing case folder: 00044\n",
      "\n",
      "Processing case folder: 00045\n",
      "\n",
      "Processing case folder: 00046\n",
      "\n",
      "Processing case folder: 00047\n",
      "\n",
      "Processing case folder: 00048\n",
      "\n",
      "Processing case folder: 00049\n",
      "\n",
      "Processing case folder: 00050\n",
      "\n",
      "Processing case folder: 00051\n",
      "\n",
      "Processing case folder: 00052\n",
      "\n",
      "Processing case folder: 00053\n",
      "\n",
      "Processing case folder: 00054\n",
      "\n",
      "Processing case folder: 00055\n",
      "\n",
      "Processing case folder: 00056\n",
      "\n",
      "Processing case folder: 00057\n",
      "\n",
      "Processing case folder: 00058\n",
      "\n",
      "Processing case folder: 00059\n",
      "\n",
      "Processing case folder: 00060\n",
      "\n",
      "Processing case folder: 00061\n",
      "\n",
      "Processing case folder: 00062\n",
      "\n",
      "Processing case folder: 00063\n",
      "\n",
      "Processing case folder: 00064\n",
      "\n",
      "Processing case folder: 00065\n",
      "\n",
      "Processing case folder: 00066\n",
      "\n",
      "Processing case folder: 00067\n",
      "\n",
      "Processing case folder: 00068\n",
      "\n",
      "Processing case folder: 00069\n",
      "\n",
      "Processing case folder: 00070\n",
      "\n",
      "Processing case folder: 00071\n",
      "\n",
      "Processing case folder: 00072\n",
      "\n",
      "Processing case folder: 00073\n",
      "\n",
      "Processing case folder: 00074\n",
      "\n",
      "Processing case folder: 00075\n",
      "\n",
      "Processing case folder: 00076\n",
      "\n",
      "Processing case folder: 00077\n",
      "\n",
      "Processing case folder: 00078\n",
      "\n",
      "Processing case folder: 00079\n",
      "\n",
      "Processing case folder: 00080\n",
      "\n",
      "Processing case folder: 00081\n",
      "\n",
      "Processing case folder: 00082\n",
      "\n",
      "Processing case folder: 00083\n",
      "\n",
      "Processing case folder: 00084\n",
      "\n",
      "Processing case folder: 00085\n",
      "\n",
      "Processing case folder: 00086\n",
      "\n",
      "Processing case folder: 00087\n",
      "\n",
      "Processing case folder: 00088\n",
      "\n",
      "Processing case folder: 00089\n",
      "\n",
      "Processing case folder: 00090\n",
      "\n",
      "Processing case folder: 00091\n",
      "\n",
      "Processing case folder: 00092\n",
      "\n",
      "Processing case folder: 00093\n",
      "\n",
      "Processing case folder: 00094\n",
      "\n",
      "Processing case folder: 00095\n",
      "\n",
      "Processing case folder: 00096\n",
      "\n",
      "Processing case folder: 00097\n",
      "\n",
      "Processing case folder: 00098\n",
      "\n",
      "Processing case folder: 00099\n",
      "\n",
      "Processing case folder: 00100\n",
      "\n",
      "Processing case folder: 00101\n",
      "\n",
      "Processing case folder: 00102\n",
      "\n",
      "Processing case folder: 00103\n",
      "\n",
      "Processing case folder: 00104\n",
      "\n",
      "Processing case folder: 00105\n",
      "\n",
      "Processing case folder: 00106\n",
      "\n",
      "Processing case folder: 00107\n",
      "\n",
      "Processing case folder: 00108\n",
      "\n",
      "Processing case folder: 00109\n",
      "\n",
      "Processing case folder: 00110\n",
      "\n",
      "Processing case folder: 00111\n",
      "\n",
      "Processing case folder: 00112\n",
      "\n",
      "Processing case folder: 00113\n",
      "\n",
      "Processing case folder: 00114\n",
      "\n",
      "Processing case folder: 00115\n",
      "\n",
      "Processing case folder: 00116\n",
      "\n",
      "Processing case folder: 00117\n",
      "\n",
      "Processing case folder: 00118\n",
      "\n",
      "Processing case folder: 00119\n",
      "\n",
      "Processing case folder: 00120\n",
      "\n",
      "Processing case folder: 00121\n",
      "\n",
      "Processing case folder: 00122\n",
      "\n",
      "Processing case folder: 00123\n",
      "\n",
      "Processing case folder: 00124\n",
      "\n",
      "Processing case folder: 00125\n",
      "\n",
      "Processing case folder: 00126\n",
      "\n",
      "Processing case folder: 00127\n",
      "\n",
      "Processing case folder: 00128\n",
      "\n",
      "Processing case folder: 00129\n",
      "\n",
      "Processing case folder: 00130\n",
      "\n",
      "Processing case folder: 00131\n",
      "\n",
      "Processing case folder: 00132\n",
      "\n",
      "Processing case folder: 00133\n",
      "\n",
      "Processing case folder: 00134\n",
      "\n",
      "Processing case folder: 00135\n",
      "\n",
      "Processing case folder: 00136\n",
      "\n",
      "Processing case folder: 00137\n",
      "\n",
      "Processing case folder: 00138\n",
      "\n",
      "Processing case folder: 00139\n",
      "\n",
      "Processing case folder: 00140\n",
      "\n",
      "Processing case folder: 00141\n",
      "\n",
      "Processing case folder: 00142\n",
      "\n",
      "Processing case folder: 00143\n",
      "\n",
      "Processing case folder: 00144\n",
      "\n",
      "Processing case folder: 00145\n",
      "\n",
      "Processing case folder: 00146\n",
      "\n",
      "Processing case folder: 00147\n",
      "\n",
      "Processing case folder: 00148\n",
      "\n",
      "Processing case folder: 00149\n",
      "\n",
      "Processing case folder: 00150\n",
      "\n",
      "Processing case folder: 00151\n",
      "\n",
      "Processing case folder: 00152\n",
      "\n",
      "Processing case folder: 00153\n",
      "\n",
      "Processing case folder: 00154\n",
      "\n",
      "Processing case folder: 00155\n",
      "\n",
      "Processing case folder: 00156\n",
      "\n",
      "Processing case folder: 00157\n",
      "\n",
      "Processing case folder: 00158\n",
      "\n",
      "Processing case folder: 00159\n",
      "\n",
      "Processing case folder: 00160\n",
      "\n",
      "Processing case folder: 00161\n",
      "\n",
      "Processing case folder: 00162\n",
      "\n",
      "Processing case folder: 00163\n",
      "\n",
      "Processing case folder: 00164\n",
      "\n",
      "Processing case folder: 00165\n",
      "\n",
      "Processing case folder: 00166\n",
      "\n",
      "Processing case folder: 00167\n",
      "\n",
      "Processing case folder: 00168\n",
      "\n",
      "Processing case folder: 00169\n",
      "\n",
      "Processing case folder: 00170\n",
      "\n",
      "Processing case folder: 00171\n",
      "\n",
      "Processing case folder: 00172\n",
      "\n",
      "Processing case folder: 00173\n",
      "\n",
      "Processing case folder: 00174\n",
      "\n",
      "Processing case folder: 00175\n",
      "\n",
      "Processing case folder: 00176\n",
      "\n",
      "Processing case folder: 00177\n",
      "\n",
      "Processing case folder: 00178\n",
      "\n",
      "Processing case folder: 00179\n",
      "\n",
      "Processing case folder: 00180\n",
      "\n",
      "Processing case folder: 00181\n",
      "\n",
      "Processing case folder: 00182\n",
      "\n",
      "Processing case folder: 00183\n",
      "\n",
      "Processing case folder: 00184\n",
      "\n",
      "Processing case folder: 00185\n",
      "\n",
      "Processing case folder: 00186\n",
      "\n",
      "Processing case folder: 00187\n",
      "\n",
      "Processing case folder: 00188\n",
      "\n",
      "Processing case folder: 00189\n",
      "\n",
      "Processing case folder: 00190\n",
      "\n",
      "Processing case folder: 00191\n",
      "\n",
      "Processing case folder: 00192\n",
      "\n",
      "Processing case folder: 00193\n",
      "\n",
      "Processing case folder: 00194\n",
      "\n",
      "Processing case folder: 00195\n",
      "\n",
      "Processing case folder: 00196\n",
      "\n",
      "Processing case folder: 00197\n",
      "\n",
      "Processing case folder: 00198\n",
      "\n",
      "Processing case folder: 00199\n",
      "\n",
      "Processing case folder: 00200\n",
      "\n",
      "Processing case folder: 00201\n",
      "\n",
      "Processing case folder: 00202\n",
      "\n",
      "Processing case folder: 00203\n",
      "\n",
      "Processing case folder: 00204\n",
      "\n",
      "Processing case folder: 00205\n",
      "\n",
      "Processing case folder: 00206\n",
      "\n",
      "Processing case folder: 00207\n",
      "\n",
      "Processing case folder: 00208\n",
      "\n",
      "Processing case folder: 00209\n",
      "\n",
      "Processing case folder: 00210\n",
      "\n",
      "Processing case folder: 00211\n",
      "\n",
      "Processing case folder: 00212\n",
      "\n",
      "Processing case folder: 00213\n",
      "\n",
      "Processing case folder: 00214\n",
      "\n",
      "Processing case folder: 00215\n",
      "\n",
      "Processing case folder: 00216\n",
      "\n",
      "Processing case folder: 00217\n",
      "\n",
      "Processing case folder: 00218\n",
      "\n",
      "Processing case folder: 00219\n",
      "\n",
      "Processing case folder: 00220\n",
      "\n",
      "Processing case folder: 00221\n",
      "\n",
      "Processing case folder: 00222\n",
      "\n",
      "Processing case folder: 00223\n",
      "\n",
      "Processing case folder: 00224\n",
      "\n",
      "Processing case folder: 00225\n",
      "\n",
      "Processing case folder: 00226\n",
      "\n",
      "Processing case folder: 00227\n",
      "\n",
      "Processing case folder: 00228\n",
      "\n",
      "Processing case folder: 00229\n",
      "\n",
      "Processing case folder: 00230\n",
      "\n",
      "Processing case folder: 00231\n",
      "\n",
      "Processing case folder: 00232\n",
      "\n",
      "Processing case folder: 00233\n",
      "\n",
      "Processing case folder: 00234\n",
      "\n",
      "Processing case folder: 00235\n",
      "\n",
      "Processing case folder: 00236\n",
      "\n",
      "Processing case folder: 00237\n",
      "\n",
      "Processing case folder: 00238\n",
      "\n",
      "Processing case folder: 00239\n",
      "\n",
      "Processing case folder: 00240\n",
      "\n",
      "Processing case folder: 00241\n",
      "\n",
      "Processing case folder: 00242\n",
      "\n",
      "Processing case folder: 00243\n",
      "\n",
      "Processing case folder: 00244\n",
      "\n",
      "Processing case folder: 00245\n",
      "\n",
      "Processing case folder: 00246\n",
      "\n",
      "Processing case folder: 00247\n",
      "\n",
      "Processing case folder: 00248\n",
      "\n",
      "Processing case folder: 00249\n",
      "\n",
      "Processing case folder: 00250\n",
      "\n",
      "Processing case folder: 00251\n",
      "\n",
      "Processing case folder: 00252\n",
      "\n",
      "Processing case folder: 00253\n",
      "\n",
      "Processing case folder: 00254\n",
      "\n",
      "Processing case folder: 00255\n",
      "\n",
      "Processing case folder: 00256\n",
      "\n",
      "Processing case folder: 00257\n",
      "\n",
      "Processing case folder: 00258\n",
      "\n",
      "Processing case folder: 00259\n",
      "\n",
      "Processing case folder: 00260\n",
      "\n",
      "Processing case folder: 00261\n",
      "\n",
      "Processing case folder: 00262\n",
      "\n",
      "Processing case folder: 00263\n",
      "\n",
      "Processing case folder: 00264\n",
      "\n",
      "Processing case folder: 00265\n",
      "\n",
      "Processing case folder: 00266\n",
      "\n",
      "Processing case folder: 00267\n",
      "\n",
      "Processing case folder: 00268\n",
      "\n",
      "Processing case folder: 00269\n",
      "\n",
      "Processing case folder: 00270\n",
      "\n",
      "Processing case folder: 00271\n",
      "\n",
      "Processing case folder: 00272\n",
      "\n",
      "Processing case folder: 00273\n",
      "\n",
      "Processing case folder: 00274\n",
      "\n",
      "Processing case folder: 00275\n",
      "\n",
      "Processing case folder: 00276\n",
      "\n",
      "Processing case folder: 00277\n",
      "\n",
      "Processing case folder: 00278\n",
      "\n",
      "Processing case folder: 00279\n",
      "\n",
      "Processing case folder: 00280\n",
      "\n",
      "Processing case folder: 00281\n",
      "\n",
      "Processing case folder: 00282\n",
      "\n",
      "Processing case folder: 00283\n",
      "\n",
      "Processing case folder: 00284\n",
      "\n",
      "Processing case folder: 00285\n",
      "\n",
      "Processing case folder: 00286\n",
      "\n",
      "Processing case folder: 00287\n",
      "\n",
      "Processing case folder: 00288\n",
      "\n",
      "Processing case folder: 00289\n",
      "\n",
      "Processing case folder: 00290\n",
      "\n",
      "Processing case folder: 00291\n",
      "\n",
      "Processing case folder: 00292\n",
      "\n",
      "Processing case folder: 00293\n",
      "\n",
      "Processing case folder: 00294\n",
      "\n",
      "Processing case folder: 00295\n",
      "\n",
      "Processing case folder: 00296\n",
      "\n",
      "Processing case folder: 00297\n",
      "\n",
      "Processing case folder: 00298\n",
      "\n",
      "Processing case folder: 00299\n",
      "\n",
      "Processing case folder: 00400\n",
      "\n",
      "Processing case folder: 00401\n",
      "\n",
      "Processing case folder: 00402\n",
      "\n",
      "Processing case folder: 00403\n",
      "\n",
      "Processing case folder: 00404\n",
      "\n",
      "Processing case folder: 00405\n",
      "\n",
      "Processing case folder: 00406\n",
      "\n",
      "Processing case folder: 00407\n",
      "\n",
      "Processing case folder: 00408\n",
      "\n",
      "Processing case folder: 00409\n",
      "\n",
      "Processing case folder: 00410\n",
      "\n",
      "Processing case folder: 00411\n",
      "\n",
      "Processing case folder: 00412\n",
      "\n",
      "Processing case folder: 00413\n",
      "\n",
      "Processing case folder: 00414\n",
      "\n",
      "Processing case folder: 00415\n",
      "\n",
      "Processing case folder: 00416\n",
      "\n",
      "Processing case folder: 00417\n",
      "\n",
      "Processing case folder: 00418\n",
      "\n",
      "Processing case folder: 00419\n",
      "\n",
      "Processing case folder: 00420\n",
      "\n",
      "Processing case folder: 00421\n",
      "\n",
      "Processing case folder: 00422\n",
      "\n",
      "Processing case folder: 00423\n",
      "\n",
      "Processing case folder: 00424\n",
      "\n",
      "Processing case folder: 00425\n",
      "\n",
      "Processing case folder: 00426\n",
      "\n",
      "Processing case folder: 00427\n",
      "\n",
      "Processing case folder: 00428\n",
      "\n",
      "Processing case folder: 00429\n",
      "\n",
      "Processing case folder: 00430\n",
      "\n",
      "Processing case folder: 00431\n",
      "\n",
      "Processing case folder: 00432\n",
      "\n",
      "Processing case folder: 00433\n",
      "\n",
      "Processing case folder: 00434\n",
      "\n",
      "Processing case folder: 00435\n",
      "\n",
      "Processing case folder: 00436\n",
      "\n",
      "Processing case folder: 00437\n",
      "\n",
      "Processing case folder: 00438\n",
      "\n",
      "Processing case folder: 00439\n",
      "\n",
      "Processing case folder: 00440\n",
      "\n",
      "Processing case folder: 00441\n",
      "\n",
      "Processing case folder: 00442\n",
      "\n",
      "Processing case folder: 00443\n",
      "\n",
      "Processing case folder: 00444\n",
      "\n",
      "Processing case folder: 00445\n",
      "\n",
      "Processing case folder: 00446\n",
      "\n",
      "Processing case folder: 00447\n",
      "\n",
      "Processing case folder: 00448\n",
      "\n",
      "Processing case folder: 00449\n",
      "\n",
      "Processing case folder: 00450\n",
      "\n",
      "Processing case folder: 00451\n",
      "\n",
      "Processing case folder: 00452\n",
      "\n",
      "Processing case folder: 00453\n",
      "\n",
      "Processing case folder: 00454\n",
      "\n",
      "Processing case folder: 00455\n",
      "\n",
      "Processing case folder: 00456\n",
      "\n",
      "Processing case folder: 00457\n",
      "\n",
      "Processing case folder: 00458\n",
      "\n",
      "Processing case folder: 00459\n",
      "\n",
      "Processing case folder: 00460\n",
      "\n",
      "Processing case folder: 00461\n",
      "\n",
      "Processing case folder: 00462\n",
      "\n",
      "Processing case folder: 00463\n",
      "\n",
      "Processing case folder: 00464\n",
      "\n",
      "Processing case folder: 00465\n",
      "\n",
      "Processing case folder: 00466\n",
      "\n",
      "Processing case folder: 00467\n",
      "\n",
      "Processing case folder: 00468\n",
      "\n",
      "Processing case folder: 00469\n",
      "\n",
      "Processing case folder: 00470\n",
      "\n",
      "Processing case folder: 00471\n",
      "\n",
      "Processing case folder: 00472\n",
      "\n",
      "Processing case folder: 00473\n",
      "\n",
      "Processing case folder: 00474\n",
      "\n",
      "Processing case folder: 00475\n",
      "\n",
      "Processing case folder: 00476\n",
      "\n",
      "Processing case folder: 00477\n",
      "\n",
      "Processing case folder: 00478\n",
      "\n",
      "Processing case folder: 00479\n",
      "\n",
      "Processing case folder: 00480\n",
      "\n",
      "Processing case folder: 00481\n",
      "\n",
      "Processing case folder: 00482\n",
      "\n",
      "Processing case folder: 00483\n",
      "\n",
      "Processing case folder: 00484\n",
      "\n",
      "Processing case folder: 00485\n",
      "\n",
      "Processing case folder: 00486\n",
      "\n",
      "Processing case folder: 00487\n",
      "\n",
      "Processing case folder: 00488\n",
      "\n",
      "Processing case folder: 00489\n",
      "\n",
      "Processing case folder: 00490\n",
      "\n",
      "Processing case folder: 00491\n",
      "\n",
      "Processing case folder: 00492\n",
      "\n",
      "Processing case folder: 00493\n",
      "\n",
      "Processing case folder: 00494\n",
      "\n",
      "Processing case folder: 00495\n",
      "\n",
      "Processing case folder: 00496\n",
      "\n",
      "Processing case folder: 00497\n",
      "\n",
      "Processing case folder: 00498\n",
      "\n",
      "Processing case folder: 00499\n",
      "\n",
      "Processing case folder: 00500\n",
      "\n",
      "Processing case folder: 00501\n",
      "\n",
      "Processing case folder: 00502\n",
      "\n",
      "Processing case folder: 00503\n",
      "\n",
      "Processing case folder: 00504\n",
      "\n",
      "Processing case folder: 00505\n",
      "\n",
      "Processing case folder: 00506\n",
      "\n",
      "Processing case folder: 00507\n",
      "\n",
      "Processing case folder: 00508\n",
      "\n",
      "Processing case folder: 00509\n",
      "\n",
      "Processing case folder: 00510\n",
      "\n",
      "Processing case folder: 00511\n",
      "\n",
      "Processing case folder: 00512\n",
      "\n",
      "Processing case folder: 00513\n",
      "\n",
      "Processing case folder: 00514\n",
      "\n",
      "Processing case folder: 00515\n",
      "\n",
      "Processing case folder: 00516\n",
      "\n",
      "Processing case folder: 00517\n",
      "\n",
      "Processing case folder: 00518\n",
      "\n",
      "Processing case folder: 00519\n",
      "\n",
      "Processing case folder: 00520\n",
      "\n",
      "Processing case folder: 00521\n",
      "\n",
      "Processing case folder: 00522\n",
      "\n",
      "Processing case folder: 00523\n",
      "\n",
      "Processing case folder: 00524\n",
      "\n",
      "Processing case folder: 00525\n",
      "\n",
      "Processing case folder: 00526\n",
      "\n",
      "Processing case folder: 00527\n",
      "\n",
      "Processing case folder: 00528\n",
      "\n",
      "Processing case folder: 00529\n",
      "\n",
      "Processing case folder: 00530\n",
      "\n",
      "Processing case folder: 00531\n",
      "\n",
      "Processing case folder: 00532\n",
      "\n",
      "Processing case folder: 00533\n",
      "\n",
      "Processing case folder: 00534\n",
      "\n",
      "Processing case folder: 00535\n",
      "\n",
      "Processing case folder: 00536\n",
      "\n",
      "Processing case folder: 00537\n",
      "\n",
      "Processing case folder: 00538\n",
      "\n",
      "Processing case folder: 00539\n",
      "\n",
      "Processing case folder: 00540\n",
      "\n",
      "Processing case folder: 00541\n",
      "\n",
      "Processing case folder: 00542\n",
      "\n",
      "Processing case folder: 00543\n",
      "\n",
      "Processing case folder: 00544\n",
      "\n",
      "Processing case folder: 00545\n",
      "\n",
      "Processing case folder: 00546\n",
      "\n",
      "Processing case folder: 00547\n",
      "\n",
      "Processing case folder: 00548\n",
      "\n",
      "Processing case folder: 00549\n",
      "\n",
      "Processing case folder: 00550\n",
      "\n",
      "Processing case folder: 00551\n",
      "\n",
      "Processing case folder: 00552\n",
      "\n",
      "Processing case folder: 00553\n",
      "\n",
      "Processing case folder: 00554\n",
      "\n",
      "Processing case folder: 00555\n",
      "\n",
      "Processing case folder: 00556\n",
      "\n",
      "Processing case folder: 00557\n",
      "\n",
      "Processing case folder: 00558\n",
      "\n",
      "Processing case folder: 00559\n",
      "\n",
      "Processing case folder: 00560\n",
      "\n",
      "Processing case folder: 00561\n",
      "\n",
      "Processing case folder: 00562\n",
      "\n",
      "Processing case folder: 00563\n",
      "\n",
      "Processing case folder: 00564\n",
      "\n",
      "Processing case folder: 00565\n",
      "\n",
      "Processing case folder: 00566\n",
      "\n",
      "Processing case folder: 00567\n",
      "\n",
      "Processing case folder: 00568\n",
      "\n",
      "Processing case folder: 00569\n",
      "\n",
      "Processing case folder: 00570\n",
      "\n",
      "Processing case folder: 00571\n",
      "\n",
      "Processing case folder: 00572\n",
      "\n",
      "Processing case folder: 00573\n",
      "\n",
      "Processing case folder: 00574\n",
      "\n",
      "Processing case folder: 00575\n",
      "\n",
      "Processing case folder: 00576\n",
      "\n",
      "Processing case folder: 00577\n",
      "\n",
      "Processing case folder: 00578\n",
      "\n",
      "Processing case folder: 00579\n",
      "\n",
      "Processing case folder: 00580\n",
      "\n",
      "Processing case folder: 00581\n",
      "\n",
      "Processing case folder: 00582\n",
      "\n",
      "Processing case folder: 00583\n",
      "\n",
      "Processing case folder: 00584\n",
      "\n",
      "Processing case folder: 00585\n",
      "\n",
      "Processing case folder: 00586\n",
      "\n",
      "Processing case folder: 00587\n",
      "\n",
      "Processing case folder: 00588\n",
      "\n",
      "All files seem to be in order!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "\n",
    "def check_files(images_path, masks_path):\n",
    "    issues_found = False\n",
    "    \n",
    "    # Get all case folders\n",
    "    image_case_folders = sorted(os.listdir(images_path))\n",
    "    mask_case_folders = sorted(os.listdir(masks_path))\n",
    "\n",
    "    print(\"Checking case folders...\")\n",
    "\n",
    "    # Ensure that the case folder structure matches in both images and masks\n",
    "    if image_case_folders != mask_case_folders:\n",
    "        print(\"Mismatch between image and mask folder structures!\")\n",
    "        return\n",
    "    \n",
    "    for case_folder in image_case_folders:\n",
    "        print(f\"\\nProcessing case folder: {case_folder}\")\n",
    "        \n",
    "        image_case_path = os.path.join(images_path, case_folder)\n",
    "        mask_case_path = os.path.join(masks_path, case_folder)\n",
    "        \n",
    "        # Ensure that both the image and mask directories exist\n",
    "        if not os.path.isdir(image_case_path):\n",
    "            print(f\"Missing image folder: {image_case_path}\")\n",
    "            issues_found = True\n",
    "            continue\n",
    "        if not os.path.isdir(mask_case_path):\n",
    "            print(f\"Missing mask folder: {mask_case_path}\")\n",
    "            issues_found = True\n",
    "            continue\n",
    "        \n",
    "        # Check for image files in the folder\n",
    "        image_files = [f for f in os.listdir(image_case_path) if f.endswith('.nii.gz')]\n",
    "        mask_files = [f for f in os.listdir(mask_case_path) if f.endswith('.nii.gz')]\n",
    "\n",
    "        # Check if image files are present\n",
    "        if not image_files:\n",
    "            print(f\"No image files found in {image_case_path}\")\n",
    "            issues_found = True\n",
    "        else:\n",
    "            for image_file in image_files:\n",
    "                image_path = os.path.join(image_case_path, image_file)\n",
    "                #print(f\"  Checking image file: {image_file}\")\n",
    "                try:\n",
    "                    image = nib.load(image_path)\n",
    "                    if image.get_fdata().size == 0:\n",
    "                        print(f\"  Empty image data in {image_path}\")\n",
    "                        issues_found = True\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error loading image {image_path}: {str(e)}\")\n",
    "                    issues_found = True\n",
    "\n",
    "        # Check if mask files are present\n",
    "        if not mask_files:\n",
    "            print(f\"No mask files found in {mask_case_path}\")\n",
    "            issues_found = True\n",
    "        else:\n",
    "            for mask_file in mask_files:\n",
    "                mask_path = os.path.join(mask_case_path, mask_file)\n",
    "                #print(f\"  Checking mask file: {mask_file}\")\n",
    "                try:\n",
    "                    mask = nib.load(mask_path)\n",
    "                    if mask.get_fdata().size == 0:\n",
    "                        print(f\"  Empty mask data in {mask_path}\")\n",
    "                        issues_found = True\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error loading mask {mask_path}: {str(e)}\")\n",
    "                    issues_found = True\n",
    "\n",
    "    if not issues_found:\n",
    "        print(\"\\nAll files seem to be in order!\")\n",
    "\n",
    "# Example usage:\n",
    "images_path = \"E:/kits23/2d_slices/images/\"\n",
    "masks_path = \"E:/kits23/2d_slices/masks/\"\n",
    "check_files(images_path, masks_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folder: 00000\n",
      "Image shape: (1, 512, 512)\n",
      "Mask shape: (1, 512, 512)\n",
      "--------------------------------------------------\n",
      "Case Folder: 00001\n",
      "Image shape: (1, 512, 512)\n",
      "Mask shape: (1, 512, 512)\n",
      "--------------------------------------------------\n",
      "Case Folder: 00002\n",
      "Image shape: (1, 512, 512)\n",
      "Mask shape: (1, 512, 512)\n",
      "--------------------------------------------------\n",
      "Case Folder: 00003\n",
      "Image shape: (1, 512, 512)\n",
      "Mask shape: (1, 512, 512)\n",
      "--------------------------------------------------\n",
      "Case Folder: 00004\n",
      "Image shape: (1, 512, 512)\n",
      "Mask shape: (1, 512, 512)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "\n",
    "# Define the paths\n",
    "images_path = \"E:/kits23/2d_slices/images/\"\n",
    "masks_path = \"E:/kits23/2d_slices/masks/\"\n",
    "\n",
    "# Function to check dimensions of images and masks\n",
    "def check_image_mask_dimensions(images_path, masks_path):\n",
    "    image_case_folders = sorted(os.listdir(images_path))  # Sort image case folders\n",
    "    mask_case_folders = sorted(os.listdir(masks_path))    # Sort mask case folders\n",
    "\n",
    "    # Loop through a few cases to check dimensions\n",
    "    for case_folder in image_case_folders[:5]:  # Check first 5 cases, adjust as needed\n",
    "        image_case_path = os.path.join(images_path, case_folder)\n",
    "        mask_case_path = os.path.join(masks_path, case_folder)\n",
    "\n",
    "        # Get image and mask files inside the case folder (they should be .nii.gz files)\n",
    "        image_files = sorted([f for f in os.listdir(image_case_path) if f.endswith('.nii.gz')])\n",
    "        mask_files = sorted([f for f in os.listdir(mask_case_path) if f.endswith('.nii.gz')])\n",
    "\n",
    "        if not image_files or not mask_files:\n",
    "            print(f\"Warning: No valid .nii.gz files found in {image_case_path} or {mask_case_path}\")\n",
    "            continue\n",
    "\n",
    "        # Load image and mask (assuming 1:1 correspondence between slices)\n",
    "        image = nib.load(os.path.join(image_case_path, image_files[0])).get_fdata()\n",
    "        mask = nib.load(os.path.join(mask_case_path, mask_files[0])).get_fdata()\n",
    "\n",
    "        # Print the dimensions\n",
    "        print(f\"Case Folder: {case_folder}\")\n",
    "        print(f\"Image shape: {image.shape}\")\n",
    "        print(f\"Mask shape: {mask.shape}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Check dimensions of images and masks\n",
    "check_image_mask_dimensions(images_path, masks_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1/5...\n",
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [235, 1, 1, 512, 512] at entry 0 and [265, 1, 1, 512, 512] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 186\u001b[0m\n\u001b[0;32m    183\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Start cross-validation\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mE:/kits23/2d_slices/images/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mE:/kits23/2d_slices/masks/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 179\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(image_dir, mask_dir, k, num_epochs, lr)\u001b[0m\n\u001b[0;32m    176\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m fold \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[16], line 122\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    120\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    121\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    123\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    124\u001b[0m     images, masks \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), masks\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [235, 1, 1, 512, 512] at entry 0 and [265, 1, 1, 512, 512] at entry 1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dataset class for loading the 2D slices and masks\n",
    "class KidneyDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, case_folders, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.case_folders = case_folders\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.case_folders)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        case_folder = self.case_folders[idx]\n",
    "        # Load images and masks for each case folder\n",
    "        images = sorted(os.listdir(os.path.join(self.image_dir, case_folder)))\n",
    "        masks = sorted(os.listdir(os.path.join(self.mask_dir, case_folder)))\n",
    "        \n",
    "        # Read the .nii.gz files and convert them to numpy arrays\n",
    "        image_stack = []\n",
    "        mask_stack = []\n",
    "        for img, mask in zip(images, masks):\n",
    "            img_data = nib.load(os.path.join(self.image_dir, case_folder, img)).get_fdata()\n",
    "            mask_data = nib.load(os.path.join(self.mask_dir, case_folder, mask)).get_fdata()\n",
    "            image_stack.append(img_data)\n",
    "            mask_stack.append(mask_data)\n",
    "        \n",
    "        # Convert lists to numpy arrays and ensure correct shape\n",
    "        image_stack = np.stack(image_stack, axis=0)\n",
    "        mask_stack = np.stack(mask_stack, axis=0)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        image_tensor = torch.tensor(image_stack, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "        mask_tensor = torch.tensor(mask_stack, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "            mask_tensor = self.transform(mask_tensor)\n",
    "        \n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "# Model architecture (already provided by you)\n",
    "class EncoderEfficientNetB5(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(EncoderEfficientNetB5, self).__init__()\n",
    "        self.encoder = EfficientNet.from_pretrained('efficientnet-b5') if pretrained else EfficientNet.from_name('efficientnet-b5')\n",
    "        self.blocks = self.encoder._blocks\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        x = self.encoder._conv_stem(x)\n",
    "        x = self.encoder._bn0(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "        return features\n",
    "\n",
    "class DecoderFPN(nn.Module):\n",
    "    def __init__(self, encoder_channels, decoder_channels=256):\n",
    "        super(DecoderFPN, self).__init__()\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        self.lat_convs = nn.ModuleList()\n",
    "        for in_channels in encoder_channels:\n",
    "            self.lat_convs.append(nn.Conv2d(in_channels, decoder_channels, kernel_size=1))\n",
    "            self.up_convs.append(nn.Conv2d(decoder_channels, decoder_channels, kernel_size=3, padding=1))\n",
    "    \n",
    "    def forward(self, encoder_features):\n",
    "        x = self.lat_convs[-1](encoder_features[-1])\n",
    "        outputs = [x]\n",
    "        for i in range(len(encoder_features) - 2, -1, -1):\n",
    "            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            lateral = self.lat_convs[i](encoder_features[i])\n",
    "            x = x + lateral\n",
    "            x = self.up_convs[i](x)\n",
    "            outputs.append(x)\n",
    "        return outputs[::-1]\n",
    "\n",
    "class EfficientNetFPN(nn.Module):\n",
    "    def __init__(self, num_classes, encoder_channels, decoder_channels=256):\n",
    "        super(EfficientNetFPN, self).__init__()\n",
    "        self.encoder = EncoderEfficientNetB5(pretrained=True)\n",
    "        self.decoder = DecoderFPN(encoder_channels, decoder_channels)\n",
    "        self.final_conv = nn.Conv2d(decoder_channels, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoder_features = self.encoder(x)\n",
    "        decoder_features = self.decoder(encoder_features)\n",
    "        x = decoder_features[0]\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "# Loss function (provided by you)\n",
    "class BCEWithLogitsLossCustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCEWithLogitsLossCustom, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return self.bce_loss(inputs, targets)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = BCEWithLogitsLossCustom()\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save the model with the best validation loss\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Model saved with val loss: {best_loss:.4f}\")\n",
    "\n",
    "# Cross-validation function\n",
    "def cross_validate(image_dir, mask_dir, k=5, num_epochs=50, lr=0.001):\n",
    "    # Get the list of all case folders\n",
    "    case_folders = sorted(os.listdir(image_dir))\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold = 1\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(case_folders):\n",
    "        print(f\"\\nTraining fold {fold}/{k}...\")\n",
    "        \n",
    "        # Create datasets and dataloaders for the current fold\n",
    "        train_folders = [case_folders[i] for i in train_idx]\n",
    "        val_folders = [case_folders[i] for i in val_idx]\n",
    "        \n",
    "        train_dataset = KidneyDataset(image_dir, mask_dir, train_folders, transform=None)\n",
    "        val_dataset = KidneyDataset(image_dir, mask_dir, val_folders, transform=None)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = EfficientNetFPN(num_classes=1, encoder_channels=[32, 16, 8, 4, 2])  # Adjust channels\n",
    "        model.to(device)\n",
    "        \n",
    "        # Train the model\n",
    "        train_model(model, train_loader, val_loader, num_epochs, lr)\n",
    "        fold += 1\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Start cross-validation\n",
    "cross_validate('E:/kits23/2d_slices/images/', 'E:/kits23/2d_slices/masks/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Visualize the first 10 images from the train_loader\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mvisualize_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m, in \u001b[0;36mvisualize_images\u001b[1;34m(dataset_loader, num_images)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Convert the image tensor to a numpy array for visualization (shape: [3, H, W] -> [H, W, 3])\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Remove the batch dimension and permute channels\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Plot the image\u001b[39;00m\n\u001b[0;32m     24\u001b[0m axes[i]\u001b[38;5;241m.\u001b[39mimshow(image\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# Cast to uint8 to make sure the image is displayed correctly\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming the KidneyDataset class is already defined and dataset is available\n",
    "# Assuming you have a dataset object created as 'train_dataset'\n",
    "\n",
    "# Create a DataLoader for the dataset (batch size of 1 since we want to view individual images)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Function to plot the first 10 images\n",
    "def visualize_images(dataset_loader, num_images=10):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))  # Create a grid for 10 images (2 rows, 5 columns)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (image, mask) in enumerate(dataset_loader):\n",
    "        if i >= num_images:\n",
    "            break\n",
    "        \n",
    "        # Convert the image tensor to a numpy array for visualization (shape: [3, H, W] -> [H, W, 3])\n",
    "        image = image.squeeze(0).permute(1, 2, 0).numpy()  # Remove the batch dimension and permute channels\n",
    "        \n",
    "        # Plot the image\n",
    "        axes[i].imshow(image.astype('uint8'))  # Cast to uint8 to make sure the image is displayed correctly\n",
    "        axes[i].axis('off')  # Remove axis ticks\n",
    "        axes[i].set_title(f\"Image {i+1}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the first 10 images from the train_loader\n",
    "visualize_images(train_loader, num_images=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/245 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [2, 1, 3, 513, 513]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 194\u001b[0m\n\u001b[0;32m    191\u001b[0m model \u001b[38;5;241m=\u001b[39m EfficientNetFPN(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, encoder_channels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m], decoder_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Start Training\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 136\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, valid_loader, num_epochs, learning_rate, checkpoint_dir, save_dir)\u001b[0m\n\u001b[0;32m    133\u001b[0m images, masks \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), masks\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m    139\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, masks)\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m, in \u001b[0;36mEfficientNetFPN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     encoder_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     decoder_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(encoder_features)\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m decoder_features[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Use the highest-resolution output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m, in \u001b[0;36mEncoderEfficientNetB5.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Store features from different stages\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_stem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Initial convolution\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39m_bn0(x)       \u001b[38;5;66;03m# Batch normalization\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\PRAGNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\efficientnet_pytorch\\utils.py:275\u001b[0m, in \u001b[0;36mConv2dStaticSamePadding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    274\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_padding(x)\n\u001b[1;32m--> 275\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [2, 1, 3, 513, 513]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model, loss function, and other necessary components\n",
    "class BCEWithLogitsLossCustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCEWithLogitsLossCustom, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return self.bce_loss(inputs, targets)\n",
    "\n",
    "# Dataset Class for 2D slices\n",
    "class KidneyDataset(Dataset):\n",
    "    def __init__(self, images_path, masks_path, transform=None):\n",
    "        self.images_path = images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.transform = transform\n",
    "        self.image_case_folders = sorted(os.listdir(images_path))  # Sort image case folders\n",
    "        self.mask_case_folders = sorted(os.listdir(masks_path))    # Sort mask case folders\n",
    "\n",
    "        # Ensure number of images and masks match\n",
    "        assert len(self.image_case_folders) == len(self.mask_case_folders), \"Mismatch between image and mask count!\"\n",
    "        \n",
    "        # Ensure folders are valid: skip folders with no corresponding mask\n",
    "        self.valid_folders = [\n",
    "            folder for folder in self.image_case_folders\n",
    "            if folder in self.mask_case_folders\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_folders)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        case_folder = self.valid_folders[idx]  # Use valid folders\n",
    "        image_case_path = os.path.join(self.images_path, case_folder)\n",
    "        mask_case_path = os.path.join(self.masks_path, case_folder)\n",
    "\n",
    "        # Get image and mask files inside the case folder (they should be .nii.gz files)\n",
    "        image_files = sorted([f for f in os.listdir(image_case_path) if f.endswith('.nii.gz')])\n",
    "        mask_files = sorted([f for f in os.listdir(mask_case_path) if f.endswith('.nii.gz')])\n",
    "\n",
    "        if not image_files or not mask_files:\n",
    "            print(f\"Warning: No files found in {image_case_path} or {mask_case_path} for index {idx}\")\n",
    "            raise ValueError(f\"No valid .nii.gz files found in {image_case_path} or {mask_case_path}\")\n",
    "\n",
    "        # Load the first image and mask files\n",
    "        image = nib.load(os.path.join(image_case_path, image_files[0])).get_fdata()\n",
    "        mask = nib.load(os.path.join(mask_case_path, mask_files[0])).get_fdata()\n",
    "\n",
    "        # Squeeze any singleton dimensions\n",
    "        image = np.squeeze(image)\n",
    "        mask = np.squeeze(mask)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        # Replicate the image to 3 channels (single-channel image to 3-channel)\n",
    "        image = image.unsqueeze(0).repeat(3, 1, 1)  # Shape (1, H, W) -> (3, H, W)\n",
    "\n",
    "        # Apply any transforms (if provided)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        # Ensure the shape is [B, C, H, W] (batch size, channels, height, width)\n",
    "        return image.unsqueeze(0), mask.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Save Checkpoint\n",
    "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir=\"checkpoints\"):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)  # Ensure directory exists\n",
    "    checkpoint_filename = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, checkpoint_filename)\n",
    "    print(f\"Checkpoint saved at {checkpoint_filename}\")\n",
    "\n",
    "# Load Checkpoint\n",
    "def load_checkpoint(model, optimizer, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Resuming training from epoch {epoch} with loss {loss}\")\n",
    "    return epoch, loss\n",
    "\n",
    "# Model Training Code\n",
    "def train_model(model, train_loader, valid_loader, num_epochs, learning_rate, checkpoint_dir=\"checkpoints\", save_dir=\"predictions\"):\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = BCEWithLogitsLossCustom()\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)  # For saving predictions\n",
    "\n",
    "    # Start training\n",
    "    epoch = 0\n",
    "    loss = float('inf')\n",
    "    \n",
    "    # Ensure checkpoints directory exists\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Try loading checkpoint if exists\n",
    "    latest_checkpoint = None\n",
    "    for checkpoint_file in os.listdir(checkpoint_dir):\n",
    "        if checkpoint_file.endswith(\".pt\"):\n",
    "            latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "\n",
    "    if latest_checkpoint:\n",
    "        epoch, loss = load_checkpoint(model, optimizer, latest_checkpoint)\n",
    "    else:\n",
    "        epoch = 0  # Start fresh\n",
    "    \n",
    "    for current_epoch in range(epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, masks in tqdm(train_loader, desc=f\"Epoch {current_epoch + 1}/{num_epochs}\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {current_epoch + 1} Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "        # Save predictions and model checkpoint\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, masks in valid_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Save the predicted images\n",
    "                for i in range(images.size(0)):\n",
    "                    pred_image = torch.sigmoid(outputs[i]).cpu().numpy()\n",
    "                    pred_image = np.squeeze(pred_image)  # Remove channel dimension\n",
    "                    pred_image = np.round(pred_image)  # Threshold for binary mask\n",
    "                    save_image(pred_image, current_epoch, i)\n",
    "\n",
    "        save_checkpoint(model, optimizer, current_epoch + 1, running_loss / len(train_loader))\n",
    "\n",
    "def save_image(image, epoch, index, save_dir=\"predictions\"):\n",
    "    file_name = os.path.join(save_dir, f\"epoch_{epoch}_pred_{index}.png\")\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.savefig(file_name)\n",
    "    plt.close()\n",
    "\n",
    "# Main Training Process\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    images_path = \"E:/kits23/2d_slices/images/\"\n",
    "    masks_path = \"E:/kits23/2d_slices/masks/\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 2\n",
    "\n",
    "    # Data Loaders\n",
    "    transform = None  # No need to convert image to tensor here\n",
    "    train_dataset = KidneyDataset(images_path, masks_path, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model\n",
    "    model = EfficientNetFPN(num_classes=1, encoder_channels=[32, 64, 128, 256, 512], decoder_channels=256).to(device)\n",
    "\n",
    "    # Start Training\n",
    "    train_model(model, train_loader, valid_loader, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "import nibabel as nib\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming the EfficientNetFPN model is already defined and imported\n",
    "# Assuming KidneyDataset class is already defined and imported\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 2\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "image_dir = 'E:/kits23/2d_slices/images/'  # Path to images\n",
    "mask_dir = 'E:/kits23/2d_slices/masks/'    # Path to masks\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the transformation (if needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize the dataset and DataLoader\n",
    "train_dataset = KidneyDataset(image_dir, mask_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Validation dataset (if you have separate validation data)\n",
    "# validation_dataset = KidneyDataset(val_image_dir, val_mask_dir, transform=transform)\n",
    "# valid_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize your model\n",
    "model = EfficientNetFPN(num_classes=1, encoder_channels=[32, 64, 128, 256, 512], decoder_channels=256).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy with logits\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for training the model\n",
    "def train_model(model, train_loader, num_epochs, learning_rate):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        # Iterate through the training data\n",
    "        for images, masks in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            outputs = torch.sigmoid(outputs)  # Sigmoid activation to get probabilities\n",
    "            predicted = (outputs > 0.5).float()  # Threshold at 0.5 to get binary mask\n",
    "            correct_predictions += (predicted == masks).sum().item()\n",
    "            total_predictions += masks.numel()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Optionally, save the model after each epoch\n",
    "        # torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # This will print True if CUDA is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
